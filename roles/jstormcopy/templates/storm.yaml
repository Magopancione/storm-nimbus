########### These MUST be filled in for a storm configuration
 storm.zookeeper.servers:
     - "{{zookeeper_host}}"

 storm.zookeeper.root: "/jstorm"

 #nimbus.host is being used by $JSTORM_HOME/bin/start.sh
 #it only support IP, please don't set hostname
 # For example
 # nimbus.host: "10.132.168.10, 10.132.168.45"
 nimbus.host: "{{nimbus_host}}"

# %JSTORM_HOME% is the jstorm home directory
 storm.local.dir: "%JSTORM_HOME%/data"

 java.library.path: "/usr/local/lib:/opt/local/lib:/usr/lib"



# if supervisor.slots.ports is null,
# the port list will be generated by cpu cores and system memory size
# for example, if there are 24 cpu cores and supervisor.slots.port.cpu.weight is 1.2
# then there are 24/1.2 ports for cpu,
# there are system_physical_memory_size/worker.memory.size ports for memory
# The final port number is min(cpu_ports, memory_port)
 supervisor.slots.ports.base: 6800
 supervisor.slots.port.cpu.weight: 1
 supervisor.slots.ports: null
#supervisor.slots.ports:
#    - 6800
#    - 6801
#    - 6802
#    - 6803

# Default disable user-define classloader
# If there are jar conflict between jstorm and application,
# please enable it
 topology.enable.classloader: false

# enable supervisor use cgroup to make resource isolation
# Before enable it, you should make sure:
# 	1. Linux version (>= 2.6.18)
# 	2. Have installed cgroup (check the file's existence:/proc/cgroups)
#	3. You should start your supervisor on root
# You can get more about cgroup:
#   http://t.cn/8s7nexU
 supervisor.enable.cgroup: false


### Netty will send multiple messages in one batch
### Setting true will improve throughput, but more latency
 storm.messaging.netty.transfer.async.batch: true

### if this setting  is true, it will use disruptor as internal queue, which size is limited
### otherwise, it will use LinkedBlockingDeque as internal queue , which size is unlimited
### generally when this setting is true, the topology will be more stable,
### but when there is a data loop flow, for example A -> B -> C -> A
### and the data flow occur blocking, please set this as false
 topology.buffer.size.limited: true

### default worker memory size, unit is byte
 worker.memory.size: 2147483648

# Metrics Monitor
# topology.performance.metrics: it is the switch flag for performance
# purpose. When it is disabled, the data of timer and histogram metrics
# will not be collected.
# topology.alimonitor.metrics.post: If it is disable, metrics data
# will only be printed to log. If it is enabled, the metrics data will be
# posted to alimonitor besides printing to log.
 topology.performance.metrics: true
 topology.alimonitor.metrics.post: false

# UI MultiCluster
# Following is an example of multicluster UI configuration
# ui.clusters:
#     - {
#         name: "jstorm",
#         zkRoot: "/jstorm",
#         zkServers:
#             [ "localhost"],
#         zkPort: 2181,
#       }
 nimbus.childopts: " -Xms2g -Xmx2g -Xmn768m -XX:PermSize=128m  -XX:SurvivorRatio=4 -XX:MaxTenuringThreshold=15 -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError  -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "
 worker.gc.childopts: " -XX:SurvivorRatio=4 -XX:MaxTenuringThreshold=15 -XX:+UseConcMarkSweepGC  -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:CMSFullGCsBeforeCompaction=5 -XX:+HeapDumpOnOutOfMemoryError -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseCMSCompactAtFullCollection -XX:CMSMaxAbortablePrecleanTime=5000 "


# Netty thread num: 0 means no limit
 storm.messaging.netty.server_worker_threads: 1
 storm.messaging.netty.client_worker_threads: 1
 storm.messaging.netty.buffer_size: 5242880 #5MB buffer
 storm.messaging.netty.max_retries: 30
 storm.messaging.netty.max_wait_ms: 1000
 storm.messaging.netty.min_wait_ms: 100
 storm.messaging.netty.disruptor: true
# If async and batch is used in netty transfer, netty will batch message
 storm.messaging.netty.transfer.async.batch: false
# If the Netty messaging layer is busy(netty internal buffer not writable), the Netty client will try to batch message as more as possible up to the size of storm.messaging.netty.transfer.batch.size bytes, otherwise it will try to flush message as soon as possible to reduce latency.
 storm.messaging.netty.transfer.batch.size: 262144
# We check with this interval that whether the Netty channel is writable and try to write pending messages if it is.
 storm.messaging.netty.flush.check.interval.ms: 1
# when netty connection is broken,
# when buffer size is more than storm.messaging.netty.buffer.threshold
# it will slow down the netty sending speed
 storm.messaging.netty.buffer.threshold: 8388608
 storm.messaging.netty.max.pending: 16
## send message with sync or async mode
 storm.messaging.netty.sync.mode: true
## when netty is in sync mode and client channel is unavailable,
## it will block sending until channel is ready
 storm.messaging.netty.async.block: true